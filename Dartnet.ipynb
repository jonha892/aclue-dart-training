{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5332,
     "status": "ok",
     "timestamp": 1691151285573,
     "user": {
      "displayName": "Jonas H.",
      "userId": "11777849874210407912"
     },
     "user_tz": -120
    },
    "id": "-l03UYXoVdou"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TN7xq2_toQSI"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1691154490042,
     "user": {
      "displayName": "Jonas H.",
      "userId": "11777849874210407912"
     },
     "user_tz": -120
    },
    "id": "-N2pIE5PVpm1"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n",
    "\n",
    "def encode_label(label):\n",
    "  if label == 'background':\n",
    "    return 0\n",
    "  elif label == 'dart':\n",
    "    return 1\n",
    "  elif label == 'anchor':\n",
    "    return 2\n",
    "  else:\n",
    "    raise ValueError(f'unknown label {label}')\n",
    "\n",
    "def collate(batch):\n",
    "  images = []\n",
    "  bboxes = []\n",
    "  labels = []\n",
    "\n",
    "  # Find the maximum number of objects in the batch\n",
    "  max_objects = max(sample[\"bboxes\"].size(0) for sample in batch)\n",
    "  #print(f\"max objects {max_objects} in batch of size {len(batch)}\")\n",
    "\n",
    "  for sample in batch:\n",
    "      image = sample[\"image\"]\n",
    "      batch_bboxes = sample[\"bboxes\"]\n",
    "      labels_data = sample[\"labels\"]\n",
    "\n",
    "      # Pad objects and labels to match the maximum object count\n",
    "      padded_bboxes = torch.cat(\n",
    "          (\n",
    "              batch_bboxes,\n",
    "              torch.zeros(\n",
    "                  max_objects - batch_bboxes.size(0), batch_bboxes.size(1)\n",
    "              ),\n",
    "          )\n",
    "      )\n",
    "      #print(f\"label size {labels_data.size()}\")\n",
    "      #print(f\"label data {labels_data}\")\n",
    "      padded_labels = torch.cat(\n",
    "          (\n",
    "              labels_data,\n",
    "              torch.zeros(max_objects - labels_data.size(0))\n",
    "          )\n",
    "      )\n",
    "\n",
    "      images.append(image)\n",
    "      bboxes.append(padded_bboxes)\n",
    "      labels.append(padded_labels)\n",
    "\n",
    "  # Convert images, objects, and labels to tensors\n",
    "  images = torch.stack(images)\n",
    "  bboxes = torch.stack(bboxes)\n",
    "  labels = torch.stack(labels)\n",
    "\n",
    "  return images, bboxes, labels\n",
    "\n",
    "class DartnetDataset(Dataset):\n",
    "  def __init__(self, info, device='cpu', img_transforms=[]):\n",
    "      self.info = info\n",
    "      self.device = device\n",
    "      self.transform = img_transforms\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.info)\n",
    "\n",
    "  def bounding_box_transform(self, bbox):\n",
    "    \"\"\"\n",
    "    in:  [[x0, y0], [x1, y1]]\n",
    "    out: [x, y, width, height]\n",
    "    \"\"\"\n",
    "    start, end = bbox\n",
    "    width = end[0] - start[0]\n",
    "    height = end[1] - start[1]\n",
    "    return [start[0], start[1], width, height]\n",
    "\n",
    "  def bounding_box_transform_end_point(self, bbox):\n",
    "    \"\"\"\n",
    "    in:  [[x0, y0], [x1, y1]]\n",
    "    out: [x0, y0, x1, y1]\n",
    "    \"\"\"\n",
    "    start, end = bbox\n",
    "    return [start[0], start[1], end[0], end[1]]\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    item = self.info.iloc[idx]\n",
    "\n",
    "    image_path = item[\"image_filename\"]\n",
    "\n",
    "    bboxes = []\n",
    "    labels = []\n",
    "    for a in item[\"annotations\"]:\n",
    "        box = a[\"bbox\"]\n",
    "\n",
    "        # TODO: temporary\n",
    "        x, y = box\n",
    "        offset = 40\n",
    "        x0 = x - offset\n",
    "        y0 = y - offset\n",
    "        x1 = x + offset\n",
    "        y1 = y + offset\n",
    "        # bb has format x0, y0, x1, y1\n",
    "        #x0, y0, x1, y1 = box\n",
    "        bboxes.append(\n",
    "            BoundingBox(\n",
    "                x1=x0,\n",
    "                y1=y0,\n",
    "                x2=x1,\n",
    "                y2=y1,\n",
    "            )\n",
    "        )\n",
    "        labels.append(a[\"label\"])\n",
    "    encoded_labels = [ encode_label(l) for l in labels ]\n",
    "    labels_tensor = torch.tensor(encoded_labels)\n",
    "\n",
    "    # load image as tensor\n",
    "    image = Image.open(image_path)\n",
    "    # Convert RGBA image to RGB\n",
    "    if image.mode == 'RGBA':\n",
    "        image = image.convert('RGB')\n",
    "\n",
    "    width, height = image.size\n",
    "    bboxes = BoundingBoxesOnImage(bboxes, shape=(height, width))\n",
    "\n",
    "    image = transforms.PILToTensor()(image).numpy().transpose((1, 2, 0))\n",
    "\n",
    "    #print(f\"bbox before {bboxes}, transform {self.transform}\")\n",
    "    img_aug, bboxes_aug = self.transform(image=image, bounding_boxes=bboxes)\n",
    "    #print(f\"bbox after {bboxes_aug}\")\n",
    "\n",
    "    img_aug = img_aug.transpose((2, 0, 1))\n",
    "    img_aug = torch.from_numpy(img_aug).to(torch.float32)\n",
    "\n",
    "    # TODO, use this encoding of bboxes\n",
    "    bbox_tensor = torch.tensor([self.bounding_box_transform_end_point(bbox) for bbox in bboxes_aug])\n",
    "\n",
    "    info = {\n",
    "        \"image_filename\": item[\"image_filename\"],\n",
    "        \"image_labels\": labels,\n",
    "        \"encoded_labels\": encoded_labels\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"image\": img_aug,\n",
    "        \"bboxes\": bbox_tensor,\n",
    "        \"labels\": labels_tensor,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "from torchvision.transforms import ToPILImage, Normalize\n",
    "\n",
    "def build_anchor_offsets(n_anchors, img_width, img_height, grid_size, bbox_width=None, bbox_height=None):\n",
    "    \"\"\"\n",
    "    Build anchor offsets.\n",
    "    In each grid cell, generate n_anchors offsets.\n",
    "    Squared and constant shape.\n",
    "    offset in form x0, y0, x1, y1\n",
    "    offset not normalized, uses img_size as base\n",
    "    \"\"\"\n",
    "\n",
    "    cell_width = img_width / grid_size\n",
    "    cell_height = img_height / grid_size\n",
    "\n",
    "    # squared anchor offsets if not set\n",
    "    if not bbox_width:\n",
    "        bbox_width = cell_width / 2\n",
    "    if not bbox_height:\n",
    "        bbox_height = cell_height / 2\n",
    "\n",
    "    step_size_x = cell_width / n_anchors\n",
    "    step_size_y = cell_height / n_anchors\n",
    "    \n",
    "    res = []\n",
    "    for x in range(grid_size):\n",
    "        s_pos_x = x * cell_width\n",
    "        for y in range(grid_size):\n",
    "            s_pos_y = y * cell_height\n",
    "            for n in range(n_anchors):\n",
    "                anchor_x_0 = s_pos_x + n * step_size_x\n",
    "                anchor_x_1 = s_pos_x + n * step_size_x + bbox_width\n",
    "                anchor_y_0 = s_pos_y + n * step_size_y\n",
    "                anchor_y_1 = s_pos_y + n * step_size_y + bbox_height\n",
    "                res.append([anchor_x_0, anchor_y_0, anchor_x_1, anchor_y_1])\n",
    "    return torch.tensor(res)\n",
    "\n",
    "def show_tensor(base_img_tensor):\n",
    "    img = base_img_tensor.cpu() / 255\n",
    "    img = ToPILImage()(img)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "\n",
    "def plot_boxes(base_img_tensor, bounding_boxes):\n",
    "    img = base_img_tensor.cpu() / 255\n",
    "    img = ToPILImage()(img)\n",
    "    \n",
    "    plt.imshow(img)\n",
    "\n",
    "     # Plot bounding boxes\n",
    "    for box in bounding_boxes:\n",
    "        x0, y0, x1, y1 = box\n",
    "        width = x1 - x0\n",
    "        height = y1 - y0\n",
    "        rect = patches.Rectangle((x0, y0), width, height, linewidth=1, edgecolor='r', facecolor='none')\n",
    "        plt.gca().add_patch(rect)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_diff_boxes(base_img_tensor, boxes_lst):\n",
    "    img = base_img_tensor.cpu() / 255\n",
    "    img = ToPILImage()(img)\n",
    "    \n",
    "    plt.imshow(img)\n",
    "\n",
    "     # Plot bounding boxes\n",
    "    for (bounding_boxes, c) in boxes_lst:\n",
    "        for box in bounding_boxes:\n",
    "            x0, y0, x1, y1 = box\n",
    "            width = x1 - x0\n",
    "            height = y1 - y0\n",
    "            rect = patches.Rectangle((x0, y0), width, height, linewidth=1, edgecolor=c, facecolor='none')\n",
    "            plt.gca().add_patch(rect)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 514,
     "status": "ok",
     "timestamp": 1691154094830,
     "user": {
      "displayName": "Jonas H.",
      "userId": "11777849874210407912"
     },
     "user_tz": -120
    },
    "id": "D7odSev4VrBQ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.models as models\n",
    "from torchvision.models import resnet34, ResNet34_Weights    \n",
    "\n",
    "class DartnetSSD(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_classes,\n",
    "        num_anchors,\n",
    "        offsets,\n",
    "        resnet_base=resnet34(weights=ResNet34_Weights.IMAGENET1K_V1),\n",
    "        train_all=False\n",
    "    ):\n",
    "        super(DartnetSSD, self).__init__()\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "        self.n_anchors = num_anchors\n",
    "        print(\"SSD Net configuration: \")\n",
    "        print(\"n_classes\", n_classes)\n",
    "        print(\"num_anchors\", num_anchors)\n",
    "\n",
    "        # Remove the last fully connected layer and average pooling layer of the ResNet model\n",
    "        modules = list(resnet_base.children())[:-2]\n",
    "        self.base_model = nn.Sequential(*modules)\n",
    "        if not train_all:\n",
    "            for param in self.base_model.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.extra_layers = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, kernel_size=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.bbox_offsets = offsets\n",
    "        \n",
    "        # SSD head layers for class prediction\n",
    "        self.class_conv = nn.Conv2d(\n",
    "            512, self.n_classes * n_anchors, kernel_size=3, padding=1\n",
    "        )\n",
    "\n",
    "        # SSD head layers for box prediction\n",
    "        self.box_conv = nn.Conv2d(512, 4 * n_anchors, kernel_size=3, padding=1)\n",
    "        self.offsets = offsets\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print('Input shape', x.shape)\n",
    "\n",
    "        features = self.base_model(x)\n",
    "        #print(\"features.shape\", features.shape)\n",
    "\n",
    "        features = self.extra_layers(features)\n",
    "        #print(\"features.shape\", features.shape)\n",
    "\n",
    "        # Class prediction\n",
    "        class_scores = self.class_conv(features)\n",
    "        class_scores = class_scores.permute(0, 2, 3, 1).contiguous()\n",
    "        class_scores = class_scores.view(class_scores.size(0), -1, self.n_classes)\n",
    "\n",
    "        # Box prediction\n",
    "        box_coords = self.box_conv(features)\n",
    "        box_coords = box_coords.permute(0, 2, 3, 1).contiguous()\n",
    "        box_coords = box_coords.view(box_coords.size(0), -1, 4)\n",
    "\n",
    "        #print(\"predicted boxes\", box_coords.shape, box_coords)\n",
    "        #print(\"offset\", self.offsets.shape, self.offsets)\n",
    "        adjusted_boxes = box_coords + self.offsets\n",
    "        #print(\"adjusted boxes\", adjusted_boxes.shape, adjusted_boxes)\n",
    "\n",
    "        return adjusted_boxes, class_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 230,
     "status": "ok",
     "timestamp": 1691151314877,
     "user": {
      "displayName": "Jonas H.",
      "userId": "11777849874210407912"
     },
     "user_tz": -120
    },
    "id": "xWWMeSfBcZhS"
   },
   "outputs": [],
   "source": [
    "### Loss\n",
    "import torch\n",
    "import torchvision.ops as ops\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def match_boxes(anchor_boxes, gt_boxes, iou_threshold=0.25):\n",
    "    \"\"\"\n",
    "    Match anchor boxes to ground truth boxes based on IoU criteria.\n",
    "\n",
    "    Args:\n",
    "        anchor_boxes (Tensor): Anchor boxes with shape (N, 4), where N is the number of anchor boxes.\n",
    "        gt_boxes (Tensor): Ground truth boxes with shape (M, 4), where M is the number of ground truth boxes.\n",
    "        iou_threshold (float): IoU threshold for matching anchor boxes to ground truth boxes.\n",
    "\n",
    "    Returns:\n",
    "        matched_idxs (Tensor): Indices of ground truth boxes with most overlap, with shape (N).\n",
    "        positive_mask (Tensor): Mask indicating positive iou threshold, with shape (N,).\n",
    "    \"\"\"\n",
    "    num_anchors = anchor_boxes.size(0)\n",
    "    num_gt_boxes = gt_boxes.size(0)\n",
    "\n",
    "    if num_anchors == 0 or num_gt_boxes == 0:\n",
    "        return torch.zeros(num_anchors, dtype=torch.int64), torch.zeros(\n",
    "            num_anchors, dtype=torch.uint8\n",
    "        )\n",
    "\n",
    "    print(f\"shape of anchor_boxes: {anchor_boxes.shape}\")\n",
    "    print(f\"shape of gt_boxes: {gt_boxes.shape}\")\n",
    "    iou_matrix = ops.box_iou(anchor_boxes, gt_boxes)\n",
    "    #print(\"iou_matrix\", iou_matrix)\n",
    "\n",
    "    max_iou_values, matched_idxs = iou_matrix.max(dim=1)\n",
    "    positive_mask = max_iou_values >= iou_threshold\n",
    "\n",
    "    print(f\"shape of iou_matrix: {iou_matrix.shape}\")\n",
    "    print(f\"shape of matched_idxs: {matched_idxs.shape}\")\n",
    "    print(f\"shape of positive_mask: {positive_mask.shape}\")\n",
    "\n",
    "    return matched_idxs, positive_mask\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, prediction, label):\n",
    "        predicted_classes, predicted_boxes = prediction\n",
    "        label_classes, label_boxes = label\n",
    "\n",
    "        print(f\"shape of predicted_classes: {predicted_classes.shape}\")\n",
    "        print(f\"shape of predicted_boxes: {predicted_boxes.shape}\")\n",
    "\n",
    "        batch_matched_idxs, batch_positive_masks = [], []\n",
    "\n",
    "        batch_size = predicted_boxes.size(0)\n",
    "        print(f\"batch_size: {batch_size}\")\n",
    "        for i in range(batch_size):\n",
    "            matched_idxs, positive_mask = match_boxes(\n",
    "                predicted_boxes[i], label_boxes[i]\n",
    "            )\n",
    "            batch_matched_idxs.append(matched_idxs)\n",
    "            batch_positive_masks.append(positive_mask)\n",
    "            print(\"end iteration\", i)\n",
    "        batch_matched_idxs = torch.stack(batch_matched_idxs)\n",
    "        batch_positive_masks = torch.stack(batch_positive_masks)\n",
    "\n",
    "        print(f\"shape of batch positive masks: {batch_positive_masks.shape}\")\n",
    "\n",
    "        pos_pred_classes = predicted_classes[batch_positive_masks]\n",
    "        pos_label_classes = label_classes[batch_positive_masks]\n",
    "\n",
    "        pos_pred_boxes = predicted_boxes[batch_positive_masks]\n",
    "        pos_label_boxes = label_boxes[batch_positive_masks]\n",
    "\n",
    "        print(f\"shape of pos_pred_classes: {pos_pred_classes.shape}\")\n",
    "        print(f\"shape of pos_label_classes: {pos_label_classes.shape}\")\n",
    "        print(f\"shape of pos_pred_boxes: {pos_pred_boxes.shape}\")\n",
    "        print(f\"shape of pos_label_boxes: {pos_label_boxes.shape}\")\n",
    "\n",
    "        return self.loss(\n",
    "            pos_pred_boxes, pos_pred_classes, pos_label_boxes, pos_label_classes\n",
    "        )\n",
    "\n",
    "    def loss(self, loc_preds, cls_preds, loc_labels, cls_labels):\n",
    "        # Localization loss (Smooth L1 Loss)\n",
    "        loc_loss = F.smooth_l1_loss(loc_preds, loc_labels)\n",
    "\n",
    "        # Classification loss (Focal Loss)\n",
    "        cls_loss = self.focal_loss(cls_preds, cls_labels)\n",
    "\n",
    "        # Total loss\n",
    "        loss = loc_loss + cls_loss\n",
    "        return loss\n",
    "\n",
    "    def focal_loss(self, preds, labels):\n",
    "        # Compute one-hot encoded targets\n",
    "        one_hot = F.one_hot(labels, num_classes=preds.size(-1)).float()\n",
    "\n",
    "        # Compute probabilities and weights for focal loss\n",
    "        probs = torch.sigmoid(preds)\n",
    "        pt = torch.where(one_hot == 1, probs, 1 - probs)\n",
    "        alpha = torch.where(one_hot == 1, self.alpha, 1 - self.alpha)\n",
    "\n",
    "        # Compute focal loss\n",
    "        loss = -alpha * (1 - pt) ** self.gamma * torch.log(pt)\n",
    "\n",
    "        # Sum the loss along the class dimension\n",
    "        cls_loss = loss.sum(dim=-1)\n",
    "\n",
    "        return cls_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 204,
     "status": "ok",
     "timestamp": 1691152668376,
     "user": {
      "displayName": "Jonas H.",
      "userId": "11777849874210407912"
     },
     "user_tz": -120
    },
    "id": "_nCTpAPUgtBy"
   },
   "outputs": [],
   "source": [
    "folder = 'drive/MyDrive/' if IN_COLAB else ''\n",
    "folder += 'dartnet_data/'\n",
    "filename = folder + 'labels.json'\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "executionInfo": {
     "elapsed": 301,
     "status": "ok",
     "timestamp": 1691152743150,
     "user": {
      "displayName": "Jonas H.",
      "userId": "11777849874210407912"
     },
     "user_tz": -120
    },
    "id": "l9he9Te7cR2C",
    "outputId": "8cdc393f-9abb-4d61-aedc-9c9622854675"
   },
   "outputs": [],
   "source": [
    "###\n",
    "### Load data\n",
    "###\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "with open(filename, 'r') as f:\n",
    "    df = json.load(f)\n",
    "    df = pd.DataFrame(df['labels'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1691154332045,
     "user": {
      "displayName": "Jonas H.",
      "userId": "11777849874210407912"
     },
     "user_tz": -120
    },
    "id": "vgVSbo3GlOSb"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def fit(model, dataloaders, criterion, optimizer, scheduler, n_epochs, device='cuda'):\n",
    "  model.to(device)\n",
    "  n_train = len(dataloaders['train'].dataset)\n",
    "  n_val = len(dataloaders['val'].dataset)\n",
    "\n",
    "  print(f\"Will train for {n_epochs} epochs. {n_train} train and {n_val} val samples\")\n",
    "\n",
    "\n",
    "  for epoch in range(n_epochs):\n",
    "    for phase in ['train', 'val']:\n",
    "      running_loss = 0.0\n",
    "      samples = 0\n",
    "      dataloader = dataloaders[phase]\n",
    "\n",
    "      if phase == 'train':\n",
    "        model.train()\n",
    "      else:\n",
    "        model.eval()\n",
    "\n",
    "      for images, bboxes, labels in tqdm(dataloader):\n",
    "        images, gt_bboxes, class_labels = images.to(device), bboxes.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        #print(images.shape)\n",
    "        bboxes, scores = model(images)\n",
    "        loss = criterion((bboxes, scores), (gt_bboxes, class_labels))\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        samples += images.size(0)\n",
    "      epoch_loss = running_loss / samples\n",
    "\n",
    "      scheduler.step()\n",
    "      print(f'Epoch {epoch}-{phase}\\t: loss: {epoch_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 606,
     "referenced_widgets": [
      "b281e0c3c34c405294728e205c0690cc",
      "d19a07c66b3148a0b7c8880984f174dc",
      "e5b2500c70714d38a6a7ab793d3d98ee",
      "c6c7a170b3ac4e12a6ac97fd3c383a16",
      "cc7d7fea127c4d1fa1c9a4681b176179",
      "46a0f12da18343f79eddf1f070232fa5",
      "82b08fe5f04b4384ad3ffd2e1c4383cf",
      "2127f0e7a9fe4b4eb25a5f2f070bdbf9",
      "e9bdd7a12c6e41d196ac11075e31457b",
      "33997879aeae4c68b49d50d29b3512e6",
      "0d382f8898984f70b3aba82fe8fdc914"
     ]
    },
    "executionInfo": {
     "elapsed": 751,
     "status": "error",
     "timestamp": 1691154495213,
     "user": {
      "displayName": "Jonas H.",
      "userId": "11777849874210407912"
     },
     "user_tz": -120
    },
    "id": "nSKl7z4ZfXRQ",
    "outputId": "858527c9-7bd9-469e-9376-d2688132d8ed"
   },
   "outputs": [],
   "source": [
    "###\n",
    "### Train\n",
    "###\n",
    "import imgaug.augmenters as iaa\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18, ResNet18_Weights    \n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "\n",
    "###\n",
    "### windoof\n",
    "###\n",
    "import os\n",
    "\n",
    "workers = 0\n",
    "if os.name == 'nt':\n",
    "    workers = 0\n",
    "elif IN_COLAB:\n",
    "    workers = 4\n",
    "print(f'using {workers} workers')\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu'\n",
    "\n",
    "t_train = iaa.Sequential([\n",
    "    iaa.Resize({'height': 244, 'width': 244})\n",
    "])\n",
    "\n",
    "t_val = iaa.Sequential([\n",
    "    iaa.Resize({'height': 244, 'width': 244})\n",
    "])\n",
    "\n",
    "train_dataset = DartnetDataset(df, device, img_transforms=t_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=workers, collate_fn=collate)\n",
    "val_dataloader = DataLoader(train_dataset, batch_size=1, num_workers=workers, collate_fn=collate)\n",
    "\n",
    "dataloaders = { 'train': train_dataloader, 'val': val_dataloader }\n",
    "\n",
    "n_anchors = 3\n",
    "offsets = build_anchor_offsets(n_anchors, 244, 244, 4, bbox_width=60, bbox_height=60).to(device)\n",
    "offsets = offsets.unsqueeze(0)\n",
    "model = DartnetSSD(3, n_anchors, offsets, resnet_base=resnet18(weights=ResNet18_Weights.IMAGENET1K_V1))\n",
    "#print(summary(model,\n",
    "#              input_size=(batch_size, 3, 244, 244),\n",
    "#              col_names=[\"kernel_size\", \"input_size\", \"output_size\", \"num_params\"],\n",
    "#              row_settings=[\"var_names\"],\n",
    "#              depth=4))\n",
    "\n",
    "from loss import PushkinLoss\n",
    "criterion = PushkinLoss()\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 12, eta_min=0, last_epoch=-1)\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "fit(model=model, dataloaders=dataloaders, criterion=criterion, optimizer=optimizer, scheduler=scheduler, device=device, n_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 834
    },
    "executionInfo": {
     "elapsed": 653,
     "status": "ok",
     "timestamp": 1691153727456,
     "user": {
      "displayName": "Jonas H.",
      "userId": "11777849874210407912"
     },
     "user_tz": -120
    },
    "id": "LY2odtBriQc6",
    "outputId": "e09678bc-197b-4f79-e31c-257201a956f3"
   },
   "outputs": [],
   "source": [
    "t = train_dataset[0]\n",
    "img = t['image']\n",
    "\n",
    "\n",
    "plot_boxes(img, offsets[0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = model(img.unsqueeze(0).to('cuda'))\n",
    "r[0].shape, r[1].shape, len(offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 208,
     "status": "ok",
     "timestamp": 1691154292457,
     "user": {
      "displayName": "Jonas H.",
      "userId": "11777849874210407912"
     },
     "user_tz": -120
    },
    "id": "45Q655bzlxZH",
    "outputId": "14e15113-06a9-4d7e-8819-6cdea50f1399"
   },
   "outputs": [],
   "source": [
    "m = DartnetSSD(3, 3).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 207,
     "status": "ok",
     "timestamp": 1691154259528,
     "user": {
      "displayName": "Jonas H.",
      "userId": "11777849874210407912"
     },
     "user_tz": -120
    },
    "id": "-1JY0DBMpHTE",
    "outputId": "67849567-6226-4110-fbc8-839c4344f9a3"
   },
   "outputs": [],
   "source": [
    "inpu = torch.rand((3, 3, 244, 244)).to('cuda')\n",
    "inpu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[[1,2], [3, 4], [5,6]], [[1,2], [3, 4], [5,6]]])\n",
    "print(a.shape)\n",
    "off = torch.tensor([[1,1], [2,2], [-3, 3]])\n",
    "b = a + off\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.randn([8, 15, 4, 4])\n",
    "print(t.shape)\n",
    "t = t.permute(0, 2, 3, 1).contiguous()\n",
    "t = t.view(t.size(0), -1, 3)\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = DartnetDataset(df, device, img_transforms=t_train)\n",
    "t = ds[0]\n",
    "img = t['image'].unsqueeze(0)\n",
    "\n",
    "\n",
    "boxes, _ = model(img.to(device))\n",
    "boxes = boxes.detach().cpu()[0]\n",
    "gt_boxes = t['bboxes']\n",
    "\n",
    "print('pred', boxes.shape, boxes)\n",
    "print(\"gt\", gt_boxes.shape, gt_boxes)\n",
    "\n",
    "iou_matrix = ops.box_iou(boxes, gt_boxes)\n",
    "print(\"iou_matrix\", iou_matrix)\n",
    "\n",
    "plot_diff_boxes(img[0, :, :, :], [(boxes, 'r'), (gt_boxes, 'g')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([1,2,3,4,5,6])\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 4, 4, 0, 0, 0, 2, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0]\n",
    "t = t[idx]\n",
    "print(t)\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter = [False, False, False, False, False, False, False, False, False, False,\n",
    "        False, False, False, False,  True, False, False, False,  True, False,\n",
    "        False, False, False, False, False, False, False,  True, False, False,\n",
    "        False, False, False, False, False, False,  True,  True, False, False,\n",
    "        False, False,  True, False, False, False, False, False]\n",
    "print(len(filter))\n",
    "t = t[filter]\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import tensor\n",
    "pred = tensor([[1.0154e+02, 4.0619e+01, 1.3197e+02, 7.1142e+01],\n",
    "        [6.0996e+01, 1.2235e+02, 9.1262e+01, 1.5249e+02],\n",
    "        [1.2218e+02, 6.1443e+01, 1.5232e+02, 9.1622e+01],\n",
    "        [1.4197e+02, 1.4216e+02, 1.7283e+02, 1.7250e+02],\n",
    "        [1.8307e+02, 2.0044e-01, 2.1349e+02, 3.0378e+01],\n",
    "        [2.0313e+02, 2.0292e+01, 2.3400e+02, 5.0749e+01],\n",
    "        [1.8329e+02, 1.2221e+02, 2.1349e+02, 1.5243e+02]])\n",
    "gt = tensor([[112.1414,  51.8082, 131.8586,  78.5479],\n",
    "        [ 47.3212, 128.6849,  67.0384, 155.4247],\n",
    "        [112.1414,  51.8082, 131.8586,  78.5479],\n",
    "        [129.6404, 148.4055, 149.3576, 175.1452],\n",
    "        [195.2000,  17.0466, 214.9172,  43.7863],\n",
    "        [195.2000,  17.0466, 214.9172,  43.7863],\n",
    "        [174.9899, 134.3671, 194.7071, 161.1068]])\n",
    "l = F.smooth_l1_loss(pred, gt)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tensor([False, False, False])\n",
    "(t == False).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPvSMijZR/8VsxEcGWt8eQy",
   "gpuType": "T4",
   "mount_file_id": "1BkU-yWkE94OJLOVlPJZH1nAokCUl7mcf",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0d382f8898984f70b3aba82fe8fdc914": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2127f0e7a9fe4b4eb25a5f2f070bdbf9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "33997879aeae4c68b49d50d29b3512e6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "46a0f12da18343f79eddf1f070232fa5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "82b08fe5f04b4384ad3ffd2e1c4383cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b281e0c3c34c405294728e205c0690cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d19a07c66b3148a0b7c8880984f174dc",
       "IPY_MODEL_e5b2500c70714d38a6a7ab793d3d98ee",
       "IPY_MODEL_c6c7a170b3ac4e12a6ac97fd3c383a16"
      ],
      "layout": "IPY_MODEL_cc7d7fea127c4d1fa1c9a4681b176179"
     }
    },
    "c6c7a170b3ac4e12a6ac97fd3c383a16": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_33997879aeae4c68b49d50d29b3512e6",
      "placeholder": "​",
      "style": "IPY_MODEL_0d382f8898984f70b3aba82fe8fdc914",
      "value": " 0/1 [00:00&lt;?, ?it/s]"
     }
    },
    "cc7d7fea127c4d1fa1c9a4681b176179": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d19a07c66b3148a0b7c8880984f174dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_46a0f12da18343f79eddf1f070232fa5",
      "placeholder": "​",
      "style": "IPY_MODEL_82b08fe5f04b4384ad3ffd2e1c4383cf",
      "value": "  0%"
     }
    },
    "e5b2500c70714d38a6a7ab793d3d98ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2127f0e7a9fe4b4eb25a5f2f070bdbf9",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e9bdd7a12c6e41d196ac11075e31457b",
      "value": 0
     }
    },
    "e9bdd7a12c6e41d196ac11075e31457b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
